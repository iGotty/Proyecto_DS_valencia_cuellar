"""
Multivariate Analysis Script
=============================
An√°lisis multivariado para explorar relaciones entre variables.

Este script analiza:
1. Correlaciones entre variables num√©ricas
2. Relaciones entre variables categ√≥ricas y num√©ricas
3. An√°lisis de segmentaci√≥n
4. Pruebas de hip√≥tesis
5. An√°lisis de asociaci√≥n

Autor: Proyecto Final - Ciencia de Datos Aplicada
Fecha: 2025-10-19
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency, f_oneway, kruskal
import warnings
warnings.filterwarnings('ignore')


class MultivariateAnalyzer:
    """Clase para an√°lisis multivariado de variables"""

    def __init__(self, filepath):
        """
        Inicializa el analizador multivariado

        Parameters:
        -----------
        filepath : str
            Ruta al archivo CSV del dataset
        """
        print("="*80)
        print("AN√ÅLISIS MULTIVARIADO")
        print("="*80)
        print(f"\n[INFO] Cargando dataset desde: {filepath}")
        self.df = pd.read_csv(filepath)

        # Convertir fechas
        self.df['first_order_date'] = pd.to_datetime(self.df['first_order_date'])
        self.df['fourth_order_date'] = pd.to_datetime(self.df['fourth_order_date'])

        print(f"[OK] Dataset cargado: {self.df.shape[0]:,} filas x {self.df.shape[1]} columnas\n")

    def analyze_numeric_correlations(self):
        """An√°lisis de correlaciones entre variables num√©ricas"""
        print("\n" + "="*80)
        print("1. CORRELACIONES ENTRE VARIABLES NUM√âRICAS")
        print("="*80)

        numeric_cols = ['total_orders', 'total_orders_tmenos1', 'delta_orders', 'efo_to_four']

        # Matriz de correlaci√≥n de Pearson
        print("\nüìä Matriz de Correlaci√≥n de Pearson:")
        corr_pearson = self.df[numeric_cols].corr(method='pearson')
        print(corr_pearson.round(3).to_string())

        # Matriz de correlaci√≥n de Spearman (no param√©trica)
        print("\nüìä Matriz de Correlaci√≥n de Spearman:")
        corr_spearman = self.df[numeric_cols].corr(method='spearman')
        print(corr_spearman.round(3).to_string())

        # An√°lisis de correlaciones significativas
        print("\nüîç Correlaciones Significativas (|r| > 0.3):")
        for i in range(len(numeric_cols)):
            for j in range(i+1, len(numeric_cols)):
                var1, var2 = numeric_cols[i], numeric_cols[j]
                r_pearson = corr_pearson.loc[var1, var2]
                r_spearman = corr_spearman.loc[var1, var2]

                if abs(r_pearson) > 0.3 or abs(r_spearman) > 0.3:
                    print(f"\n   {var1} vs {var2}:")
                    print(f"      Pearson: {r_pearson:.3f} ({'Positiva' if r_pearson > 0 else 'Negativa'})")
                    print(f"      Spearman: {r_spearman:.3f} ({'Positiva' if r_spearman > 0 else 'Negativa'})")

                    # Interpretaci√≥n
                    r = abs(r_pearson)
                    if r > 0.7:
                        strength = "muy fuerte"
                    elif r > 0.5:
                        strength = "fuerte"
                    elif r > 0.3:
                        strength = "moderada"
                    else:
                        strength = "d√©bil"
                    print(f"      Interpretaci√≥n: Correlaci√≥n {strength}")

        return {'pearson': corr_pearson, 'spearman': corr_spearman}

    def analyze_categorical_numeric_relationship(self, cat_var, num_var, description=""):
        """
        Analiza la relaci√≥n entre una variable categ√≥rica y una num√©rica

        Parameters:
        -----------
        cat_var : str
            Variable categ√≥rica
        num_var : str
            Variable num√©rica
        description : str
            Descripci√≥n del an√°lisis
        """
        print("\n" + "-"*80)
        print(f"üìä {cat_var} vs {num_var}")
        if description:
            print(f"   {description}")
        print("-"*80)

        # Estad√≠sticas descriptivas por grupo
        print("\nüìà Estad√≠sticas Descriptivas por Grupo:")
        grouped = self.df.groupby(cat_var)[num_var].agg(['count', 'mean', 'median', 'std', 'min', 'max'])
        print(grouped.round(2).to_string())

        # Test de normalidad por grupo (si hay pocos grupos)
        groups = [group[num_var].values for name, group in self.df.groupby(cat_var)]

        # Test ANOVA (param√©trico) o Kruskal-Wallis (no param√©trico)
        print(f"\nüî¨ Test de Diferencias entre Grupos:")

        # ANOVA
        f_stat, p_value_anova = f_oneway(*groups)
        print(f"\n   ANOVA (param√©trico):")
        print(f"      F-estad√≠stico: {f_stat:.4f}")
        print(f"      P-valor: {p_value_anova:.4e}")
        if p_value_anova < 0.05:
            print(f"      ‚úì Hay diferencias significativas entre grupos (p < 0.05)")
        else:
            print(f"      ‚úó No hay diferencias significativas entre grupos (p >= 0.05)")

        # Kruskal-Wallis (alternativa no param√©trica)
        h_stat, p_value_kruskal = kruskal(*groups)
        print(f"\n   Kruskal-Wallis (no param√©trico):")
        print(f"      H-estad√≠stico: {h_stat:.4f}")
        print(f"      P-valor: {p_value_kruskal:.4e}")
        if p_value_kruskal < 0.05:
            print(f"      ‚úì Hay diferencias significativas entre grupos (p < 0.05)")
        else:
            print(f"      ‚úó No hay diferencias significativas entre grupos (p >= 0.05)")

        # Tama√±o del efecto (eta cuadrado)
        grand_mean = self.df[num_var].mean()
        ss_between = sum([len(group) * (group[num_var].mean() - grand_mean)**2
                          for name, group in self.df.groupby(cat_var)])
        ss_total = sum((self.df[num_var] - grand_mean)**2)
        eta_squared = ss_between / ss_total

        print(f"\n   Tama√±o del Efecto (Œ∑¬≤):")
        print(f"      Œ∑¬≤ = {eta_squared:.4f}")
        if eta_squared < 0.01:
            effect_size = "muy peque√±o"
        elif eta_squared < 0.06:
            effect_size = "peque√±o"
        elif eta_squared < 0.14:
            effect_size = "mediano"
        else:
            effect_size = "grande"
        print(f"      Interpretaci√≥n: Efecto {effect_size}")

        return {
            'grouped_stats': grouped,
            'anova': {'f_stat': f_stat, 'p_value': p_value_anova},
            'kruskal': {'h_stat': h_stat, 'p_value': p_value_kruskal},
            'eta_squared': eta_squared
        }

    def analyze_categorical_associations(self):
        """Analiza asociaciones entre variables categ√≥ricas"""
        print("\n" + "="*80)
        print("3. ASOCIACIONES ENTRE VARIABLES CATEG√ìRICAS")
        print("="*80)

        cat_vars = ['categoria_recencia', 'city_token', 'r_segment']

        results = {}

        for i in range(len(cat_vars)):
            for j in range(i+1, len(cat_vars)):
                var1, var2 = cat_vars[i], cat_vars[j]

                print(f"\nüìä {var1} vs {var2}")
                print("-"*80)

                # Tabla de contingencia
                contingency_table = pd.crosstab(self.df[var1], self.df[var2])

                # Test Chi-cuadrado
                chi2, p_value, dof, expected = chi2_contingency(contingency_table)

                print(f"\nüî¨ Test Chi-Cuadrado de Independencia:")
                print(f"   Chi¬≤ estad√≠stico: {chi2:.4f}")
                print(f"   Grados de libertad: {dof}")
                print(f"   P-valor: {p_value:.4e}")

                if p_value < 0.05:
                    print(f"   ‚úì Las variables est√°n asociadas (p < 0.05)")
                else:
                    print(f"   ‚úó Las variables son independientes (p >= 0.05)")

                # Coeficiente de Cram√©r's V (tama√±o del efecto)
                n = contingency_table.sum().sum()
                min_dim = min(contingency_table.shape[0] - 1, contingency_table.shape[1] - 1)
                cramers_v = np.sqrt(chi2 / (n * min_dim))

                print(f"\n   Cram√©r's V (tama√±o del efecto):")
                print(f"   V = {cramers_v:.4f}")
                if cramers_v < 0.1:
                    strength = "muy d√©bil"
                elif cramers_v < 0.3:
                    strength = "d√©bil"
                elif cramers_v < 0.5:
                    strength = "moderada"
                else:
                    strength = "fuerte"
                print(f"   Interpretaci√≥n: Asociaci√≥n {strength}")

                # Mostrar tabla de contingencia (solo si no es muy grande)
                if contingency_table.shape[0] <= 10 and contingency_table.shape[1] <= 10:
                    print(f"\n   Tabla de Contingencia:")
                    print(contingency_table.to_string())

                results[f"{var1}_vs_{var2}"] = {
                    'chi2': chi2,
                    'p_value': p_value,
                    'cramers_v': cramers_v,
                    'contingency_table': contingency_table
                }

        return results

    def analyze_growth_patterns(self):
        """An√°lisis de patrones de crecimiento (delta_orders)"""
        print("\n" + "="*80)
        print("4. AN√ÅLISIS DE PATRONES DE CRECIMIENTO")
        print("="*80)

        # Segmentar usuarios por crecimiento
        print("\nüìä Segmentaci√≥n por Nivel de Crecimiento:")

        self.df['growth_segment'] = pd.cut(
            self.df['delta_orders'],
            bins=[0, 4, 8, 15, float('inf')],
            labels=['Bajo (1-4)', 'Medio (5-8)', 'Alto (9-15)', 'Muy Alto (>15)']
        )

        growth_dist = self.df['growth_segment'].value_counts().sort_index()
        print("\n   Distribuci√≥n:")
        for segment, count in growth_dist.items():
            pct = (count / len(self.df) * 100)
            print(f"     {segment}: {count:,} ({pct:.1f}%)")

        # Relaci√≥n entre velocidad (efo_to_four) y crecimiento
        print("\n\nüìà Relaci√≥n entre Velocidad de Adopci√≥n (EFO-to-Four) y Crecimiento:")
        grouped = self.df.groupby('growth_segment')['efo_to_four'].agg(['mean', 'median', 'std'])
        print(grouped.round(2).to_string())

        # Test de correlaci√≥n
        corr = self.df[['efo_to_four', 'delta_orders']].corr().iloc[0, 1]
        print(f"\n   Correlaci√≥n efo_to_four vs delta_orders: {corr:.3f}")
        if abs(corr) > 0.3:
            direction = "negativa" if corr < 0 else "positiva"
            print(f"   üí° Insight: Hay una correlaci√≥n {direction} {'fuerte' if abs(corr) > 0.5 else 'moderada'}")
            if corr < 0:
                print(f"      ‚Üí Usuarios que llegan r√°pido a su 4ta orden tienden a tener MAYOR crecimiento")
            else:
                print(f"      ‚Üí Usuarios que tardan m√°s en su 4ta orden tienden a tener MAYOR crecimiento")

        return growth_dist

    def analyze_recency_impact(self):
        """Analiza el impacto de la recencia en el crecimiento"""
        print("\n" + "="*80)
        print("5. IMPACTO DE LA RECENCIA EN EL CRECIMIENTO")
        print("="*80)

        # Ordenar categor√≠as de recencia
        recency_order = ['Activo (‚â§7d)', 'Semi-Activo (8‚Äì14d)', 'Tibio (15‚Äì30d)',
                        'Fr√≠o (31‚Äì90d)', 'Perdido (>90d)']

        self.df['categoria_recencia'] = pd.Categorical(
            self.df['categoria_recencia'],
            categories=recency_order,
            ordered=True
        )

        print("\nüìä Delta de √ìrdenes por Categor√≠a de Recencia:")
        grouped = self.df.groupby('categoria_recencia')['delta_orders'].agg([
            'count', 'mean', 'median', 'std'
        ])
        print(grouped.round(2).to_string())

        print("\nüí° Insights:")
        max_growth = grouped['mean'].idxmax()
        min_growth = grouped['mean'].idxmin()
        print(f"   - Mayor crecimiento promedio: {max_growth} ({grouped.loc[max_growth, 'mean']:.2f} √≥rdenes)")
        print(f"   - Menor crecimiento promedio: {min_growth} ({grouped.loc[min_growth, 'mean']:.2f} √≥rdenes)")

        return grouped

    def analyze_segment_performance(self):
        """Analiza el desempe√±o por segmento R"""
        print("\n" + "="*80)
        print("6. DESEMPE√ëO POR SEGMENTO R")
        print("="*80)

        print("\nüìä M√©tricas Clave por Segmento R:")

        # Agrupar por segmento R
        segment_analysis = self.df.groupby('r_segment').agg({
            'uid': 'count',
            'total_orders': ['mean', 'median', 'std'],
            'delta_orders': ['mean', 'median', 'std'],
            'efo_to_four': ['mean', 'median', 'std']
        }).round(2)

        segment_analysis.columns = ['_'.join(col).strip() for col in segment_analysis.columns.values]
        print(segment_analysis.to_string())

        # Identificar el mejor segmento
        print("\nüí° Insights:")
        best_growth = self.df.groupby('r_segment')['delta_orders'].mean().idxmax()
        best_retention = self.df.groupby('r_segment')['total_orders'].mean().idxmax()
        fastest = self.df.groupby('r_segment')['efo_to_four'].mean().idxmin()

        print(f"   - Segmento con mayor crecimiento (delta): {best_growth}")
        print(f"   - Segmento con m√°s √≥rdenes totales: {best_retention}")
        print(f"   - Segmento con adopci√≥n m√°s r√°pida: {fastest}")

        return segment_analysis

    def analyze_city_performance(self):
        """Analiza el desempe√±o por ciudad"""
        print("\n" + "="*80)
        print("7. DESEMPE√ëO POR CIUDAD")
        print("="*80)

        print("\nüìä M√©tricas Clave por Ciudad:")

        # Agrupar por ciudad
        city_analysis = self.df.groupby('city_token').agg({
            'uid': 'count',
            'total_orders': ['mean', 'median'],
            'delta_orders': ['mean', 'median'],
            'efo_to_four': ['mean', 'median']
        }).round(2)

        city_analysis.columns = ['_'.join(col).strip() for col in city_analysis.columns.values]
        city_analysis = city_analysis.sort_values('delta_orders_mean', ascending=False)
        print(city_analysis.to_string())

        print("\nüí° Insights:")
        best_city = city_analysis['delta_orders_mean'].idxmax()
        print(f"   - Ciudad con mayor crecimiento promedio: {best_city}")
        print(f"   - Delta promedio: {city_analysis.loc[best_city, 'delta_orders_mean']:.2f} √≥rdenes")

        return city_analysis

    def generate_executive_summary(self):
        """Genera resumen ejecutivo del an√°lisis multivariado"""
        print("\n" + "="*80)
        print("RESUMEN EJECUTIVO - AN√ÅLISIS MULTIVARIADO")
        print("="*80)

        # Calcular algunas m√©tricas clave
        corr_efo_delta = self.df[['efo_to_four', 'delta_orders']].corr().iloc[0, 1]

        print(f"""
üéØ Hallazgos Principales:

   1. Correlaci√≥n Velocidad-Crecimiento:
      - Correlaci√≥n efo_to_four vs delta_orders: {corr_efo_delta:.3f}
      - {'Los usuarios que llegan m√°s r√°pido a su 4ta orden tienden a crecer M√ÅS' if corr_efo_delta < 0 else 'Los usuarios que tardan m√°s tienden a crecer M√ÅS'}

   2. Segmentaci√≥n por Crecimiento:
      - {(self.df['delta_orders'] <= 4).sum():,} usuarios con bajo crecimiento (1-4 √≥rdenes)
      - {((self.df['delta_orders'] > 4) & (self.df['delta_orders'] <= 8)).sum():,} usuarios con crecimiento medio (5-8 √≥rdenes)
      - {(self.df['delta_orders'] > 8).sum():,} usuarios con alto crecimiento (>8 √≥rdenes)

   3. Mejores Segmentos:
      - Segmento R con mayor crecimiento: {self.df.groupby('r_segment')['delta_orders'].mean().idxmax()}
      - Ciudad con mayor crecimiento: {self.df.groupby('city_token')['delta_orders'].mean().idxmax()}

üí° Recomendaciones Estrat√©gicas:
   1. Enfocar recursos en usuarios con alta velocidad de adopci√≥n (bajo efo_to_four)
   2. Personalizar estrategias por segmento R y ciudad
   3. Investigar qu√© diferencia a los usuarios de alto crecimiento
""")

    def run_full_analysis(self):
        """Ejecuta el an√°lisis multivariado completo"""
        print("\n")
        print("üöÄ Iniciando an√°lisis multivariado completo...\n")

        # Ejecutar an√°lisis
        corr_results = self.analyze_numeric_correlations()

        print("\n" + "="*80)
        print("2. RELACIONES CATEG√ìRICAS-NUM√âRICAS")
        print("="*80)

        # Recencia vs variables num√©ricas
        rec_orders = self.analyze_categorical_numeric_relationship(
            'categoria_recencia', 'total_orders',
            'Impacto de la recencia en el total de √≥rdenes'
        )

        rec_delta = self.analyze_categorical_numeric_relationship(
            'categoria_recencia', 'delta_orders',
            'Impacto de la recencia en el crecimiento (delta)'
        )

        # Segmento R vs variables num√©ricas
        seg_delta = self.analyze_categorical_numeric_relationship(
            'r_segment', 'delta_orders',
            'Impacto del segmento R en el crecimiento'
        )

        seg_efo = self.analyze_categorical_numeric_relationship(
            'r_segment', 'efo_to_four',
            'Impacto del segmento R en la velocidad de adopci√≥n'
        )

        # Asociaciones categ√≥ricas
        assoc_results = self.analyze_categorical_associations()

        # An√°lisis espec√≠ficos
        growth_dist = self.analyze_growth_patterns()
        recency_impact = self.analyze_recency_impact()
        segment_perf = self.analyze_segment_performance()
        city_perf = self.analyze_city_performance()

        # Resumen ejecutivo
        self.generate_executive_summary()

        print("\n" + "="*80)
        print("‚úÖ An√°lisis multivariado completado")
        print("="*80)

        return {
            'correlations': corr_results,
            'categorical_associations': assoc_results,
            'growth_distribution': growth_dist,
            'recency_impact': recency_impact,
            'segment_performance': segment_perf,
            'city_performance': city_perf
        }


if __name__ == "__main__":
    # Ruta al dataset
    DATASET_PATH = "../dataset_protegido (1).csv"

    # Crear instancia del analizador
    analyzer = MultivariateAnalyzer(DATASET_PATH)

    # Ejecutar an√°lisis completo
    results = analyzer.run_full_analysis()

    print("\nüí° Pr√≥ximos pasos recomendados:")
    print("   1. Crear visualizaciones para comunicar estos hallazgos")
    print("   2. Realizar an√°lisis de clustering basado en comportamiento")
    print("   3. Construir modelos predictivos de crecimiento")
