"""
Univariate Analysis Script
===========================
An√°lisis univariado exhaustivo de todas las variables del dataset.

Este script realiza an√°lisis estad√≠stico descriptivo y exploratorio de:
- Variables num√©ricas (total_orders, delta_orders, efo_to_four)
- Variables categ√≥ricas (country_code, categoria_recencia, city_token, r_segment)
- Variables temporales (first_order_date, fourth_order_date)

Autor: Proyecto Final - Ciencia de Datos Aplicada
Fecha: 2025-10-19
"""

import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class UnivariateAnalyzer:
    """Clase para an√°lisis univariado de variables"""

    def __init__(self, filepath):
        """
        Inicializa el analizador univariado

        Parameters:
        -----------
        filepath : str
            Ruta al archivo CSV del dataset
        """
        print("="*80)
        print("AN√ÅLISIS UNIVARIADO DE VARIABLES")
        print("="*80)
        print(f"\n[INFO] Cargando dataset desde: {filepath}")
        self.df = pd.read_csv(filepath)

        # Convertir fechas
        self.df['first_order_date'] = pd.to_datetime(self.df['first_order_date'])
        self.df['fourth_order_date'] = pd.to_datetime(self.df['fourth_order_date'])

        print(f"[OK] Dataset cargado: {self.df.shape[0]:,} filas x {self.df.shape[1]} columnas\n")

    def analyze_numeric_variable(self, column_name, description=""):
        """
        An√°lisis completo de una variable num√©rica

        Parameters:
        -----------
        column_name : str
            Nombre de la columna a analizar
        description : str
            Descripci√≥n de la variable
        """
        print("\n" + "-"*80)
        print(f"üìä Variable: {column_name}")
        if description:
            print(f"   Descripci√≥n: {description}")
        print("-"*80)

        data = self.df[column_name]

        # Estad√≠sticas descriptivas b√°sicas
        print("\nüî¢ Estad√≠sticas Descriptivas:")
        print(f"   Observaciones: {len(data):,}")
        print(f"   Media: {data.mean():.2f}")
        print(f"   Mediana: {data.median():.2f}")
        print(f"   Moda: {data.mode().values[0] if len(data.mode()) > 0 else 'N/A'}")
        print(f"   Desviaci√≥n est√°ndar: {data.std():.2f}")
        print(f"   Varianza: {data.var():.2f}")
        print(f"   M√≠nimo: {data.min()}")
        print(f"   M√°ximo: {data.max()}")
        print(f"   Rango: {data.max() - data.min()}")

        # Cuartiles
        print(f"\nüìà Cuartiles:")
        print(f"   Q1 (25%): {data.quantile(0.25):.2f}")
        print(f"   Q2 (50% - Mediana): {data.quantile(0.50):.2f}")
        print(f"   Q3 (75%): {data.quantile(0.75):.2f}")
        print(f"   IQR (Q3-Q1): {data.quantile(0.75) - data.quantile(0.25):.2f}")

        # Percentiles adicionales
        print(f"\nüéØ Percentiles Clave:")
        for p in [5, 10, 25, 50, 75, 90, 95, 99]:
            print(f"   P{p}: {data.quantile(p/100):.2f}")

        # Asimetr√≠a y curtosis
        print(f"\nüìê Forma de la Distribuci√≥n:")
        skewness = data.skew()
        kurtosis = data.kurtosis()
        print(f"   Asimetr√≠a (Skewness): {skewness:.3f}")
        if skewness > 1:
            print(f"      ‚Üí Distribuci√≥n muy sesgada a la derecha")
        elif skewness > 0.5:
            print(f"      ‚Üí Distribuci√≥n moderadamente sesgada a la derecha")
        elif skewness < -1:
            print(f"      ‚Üí Distribuci√≥n muy sesgada a la izquierda")
        elif skewness < -0.5:
            print(f"      ‚Üí Distribuci√≥n moderadamente sesgada a la izquierda")
        else:
            print(f"      ‚Üí Distribuci√≥n aproximadamente sim√©trica")

        print(f"   Curtosis (Kurtosis): {kurtosis:.3f}")
        if kurtosis > 3:
            print(f"      ‚Üí Distribuci√≥n leptoc√∫rtica (colas pesadas)")
        elif kurtosis < -1:
            print(f"      ‚Üí Distribuci√≥n platic√∫rtica (colas ligeras)")
        else:
            print(f"      ‚Üí Distribuci√≥n mesoc√∫rtica (normal)")

        # Test de normalidad
        print(f"\nüî¨ Test de Normalidad (Shapiro-Wilk):")
        if len(data) <= 5000:
            stat, p_value = stats.shapiro(data)
            print(f"   Estad√≠stico: {stat:.4f}")
            print(f"   P-valor: {p_value:.4e}")
            if p_value > 0.05:
                print(f"   ‚úì No se rechaza H0: La distribuci√≥n podr√≠a ser normal")
            else:
                print(f"   ‚úó Se rechaza H0: La distribuci√≥n NO es normal")
        else:
            print(f"   ‚ö†Ô∏è  Muestra muy grande para Shapiro-Wilk. Usando Anderson-Darling...")
            result = stats.anderson(data)
            print(f"   Estad√≠stico: {result.statistic:.4f}")
            print(f"   Valor cr√≠tico (5%): {result.critical_values[2]:.4f}")
            if result.statistic < result.critical_values[2]:
                print(f"   ‚úì La distribuci√≥n podr√≠a ser normal")
            else:
                print(f"   ‚úó La distribuci√≥n NO es normal")

        # Distribuci√≥n de frecuencias
        print(f"\nüìä Distribuci√≥n de Frecuencias (Top 15):")
        freq_dist = data.value_counts().head(15)
        for value, count in freq_dist.items():
            pct = (count / len(data) * 100)
            print(f"   {value}: {count:,} ({pct:.1f}%)")

        # Coeficiente de variaci√≥n
        cv = (data.std() / data.mean() * 100) if data.mean() != 0 else 0
        print(f"\nüìâ Coeficiente de Variaci√≥n: {cv:.2f}%")
        if cv < 15:
            print(f"   ‚Üí Baja variabilidad")
        elif cv < 30:
            print(f"   ‚Üí Variabilidad moderada")
        else:
            print(f"   ‚Üí Alta variabilidad")

        return {
            'mean': data.mean(),
            'median': data.median(),
            'std': data.std(),
            'skewness': skewness,
            'kurtosis': kurtosis,
            'cv': cv
        }

    def analyze_categorical_variable(self, column_name, description="", top_n=20):
        """
        An√°lisis completo de una variable categ√≥rica

        Parameters:
        -----------
        column_name : str
            Nombre de la columna a analizar
        description : str
            Descripci√≥n de la variable
        top_n : int
            N√∫mero de categor√≠as a mostrar
        """
        print("\n" + "-"*80)
        print(f"üìä Variable: {column_name}")
        if description:
            print(f"   Descripci√≥n: {description}")
        print("-"*80)

        data = self.df[column_name]

        # Estad√≠sticas b√°sicas
        print("\nüî¢ Estad√≠sticas Descriptivas:")
        print(f"   Observaciones: {len(data):,}")
        print(f"   Valores √∫nicos: {data.nunique()}")
        print(f"   Moda: {data.mode().values[0] if len(data.mode()) > 0 else 'N/A'}")

        # Distribuci√≥n de frecuencias
        print(f"\nüìä Distribuci√≥n de Frecuencias:")
        freq_dist = data.value_counts()

        # Mostrar todas las categor√≠as si son pocas, sino solo top N
        if data.nunique() <= top_n:
            for value, count in freq_dist.items():
                pct = (count / len(data) * 100)
                print(f"   {value}: {count:,} ({pct:.2f}%)")
        else:
            print(f"\n   Top {top_n} categor√≠as m√°s frecuentes:")
            for value, count in freq_dist.head(top_n).items():
                pct = (count / len(data) * 100)
                print(f"   {value}: {count:,} ({pct:.2f}%)")

            # Categor√≠as menos frecuentes
            print(f"\n   Bottom 10 categor√≠as menos frecuentes:")
            for value, count in freq_dist.tail(10).items():
                pct = (count / len(data) * 100)
                print(f"   {value}: {count:,} ({pct:.2f}%)")

        # Concentraci√≥n
        print(f"\nüéØ An√°lisis de Concentraci√≥n:")
        cumsum = (freq_dist / len(data) * 100).cumsum()
        categories_80 = (cumsum <= 80).sum()
        categories_90 = (cumsum <= 90).sum()
        print(f"   Categor√≠as que representan 80% de datos: {categories_80}")
        print(f"   Categor√≠as que representan 90% de datos: {categories_90}")

        # √çndice de diversidad (Shannon)
        proportions = freq_dist / len(data)
        shannon_index = -sum(proportions * np.log(proportions))
        max_shannon = np.log(data.nunique())
        shannon_evenness = shannon_index / max_shannon if max_shannon > 0 else 0

        print(f"\nüìà √çndice de Diversidad de Shannon:")
        print(f"   √çndice: {shannon_index:.3f}")
        print(f"   √çndice m√°ximo posible: {max_shannon:.3f}")
        print(f"   Equitabilidad (evenness): {shannon_evenness:.3f}")
        if shannon_evenness > 0.8:
            print(f"   ‚Üí Alta diversidad: las categor√≠as est√°n bien distribuidas")
        elif shannon_evenness > 0.5:
            print(f"   ‚Üí Diversidad moderada")
        else:
            print(f"   ‚Üí Baja diversidad: hay categor√≠as muy dominantes")

        return {
            'unique_values': data.nunique(),
            'mode': data.mode().values[0] if len(data.mode()) > 0 else None,
            'freq_dist': freq_dist,
            'shannon_index': shannon_index,
            'shannon_evenness': shannon_evenness
        }

    def analyze_temporal_variable(self, column_name, description=""):
        """
        An√°lisis de variables temporales/fechas

        Parameters:
        -----------
        column_name : str
            Nombre de la columna a analizar
        description : str
            Descripci√≥n de la variable
        """
        print("\n" + "-"*80)
        print(f"üìä Variable: {column_name}")
        if description:
            print(f"   Descripci√≥n: {description}")
        print("-"*80)

        data = pd.to_datetime(self.df[column_name])

        # Estad√≠sticas b√°sicas
        print("\nüìÖ Estad√≠sticas Descriptivas:")
        print(f"   Observaciones: {len(data):,}")
        print(f"   Fecha m√≠nima: {data.min().strftime('%Y-%m-%d')}")
        print(f"   Fecha m√°xima: {data.max().strftime('%Y-%m-%d')}")
        print(f"   Rango: {(data.max() - data.min()).days} d√≠as")

        # Distribuci√≥n por mes
        print(f"\nüìä Distribuci√≥n por Mes:")
        monthly_dist = data.dt.to_period('M').value_counts().sort_index()
        for month, count in monthly_dist.items():
            pct = (count / len(data) * 100)
            print(f"   {month}: {count:,} ({pct:.1f}%)")

        # Distribuci√≥n por d√≠a de la semana
        print(f"\nüìä Distribuci√≥n por D√≠a de la Semana:")
        dow_map = {0: 'Lunes', 1: 'Martes', 2: 'Mi√©rcoles', 3: 'Jueves',
                   4: 'Viernes', 5: 'S√°bado', 6: 'Domingo'}
        dow_dist = data.dt.dayofweek.map(dow_map).value_counts()
        dow_order = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']
        for day in dow_order:
            if day in dow_dist.index:
                count = dow_dist[day]
                pct = (count / len(data) * 100)
                print(f"   {day}: {count:,} ({pct:.1f}%)")

        return {
            'min_date': data.min(),
            'max_date': data.max(),
            'range_days': (data.max() - data.min()).days,
            'monthly_dist': monthly_dist
        }

    def run_full_analysis(self):
        """Ejecuta el an√°lisis univariado completo"""
        print("\n")
        print("üöÄ Iniciando an√°lisis univariado completo...\n")

        results = {}

        # An√°lisis de variables num√©ricas
        print("\n" + "="*80)
        print("AN√ÅLISIS DE VARIABLES NUM√âRICAS")
        print("="*80)

        results['total_orders'] = self.analyze_numeric_variable(
            'total_orders',
            'Total de √≥rdenes completadas por el usuario'
        )

        results['total_orders_tmenos1'] = self.analyze_numeric_variable(
            'total_orders_tmenos1',
            'Total de √≥rdenes en el periodo anterior (T-1)'
        )

        results['delta_orders'] = self.analyze_numeric_variable(
            'delta_orders',
            'Incremento de √≥rdenes entre periodos (delta)'
        )

        results['efo_to_four'] = self.analyze_numeric_variable(
            'efo_to_four',
            'D√≠as entre la primera y cuarta orden (velocidad de adopci√≥n)'
        )

        # An√°lisis de variables categ√≥ricas
        print("\n" + "="*80)
        print("AN√ÅLISIS DE VARIABLES CATEG√ìRICAS")
        print("="*80)

        results['country_code'] = self.analyze_categorical_variable(
            'country_code',
            'C√≥digo del pa√≠s del usuario'
        )

        results['categoria_recencia'] = self.analyze_categorical_variable(
            'categoria_recencia',
            'Categor√≠a de recencia del usuario (√∫ltima actividad)'
        )

        results['city_token'] = self.analyze_categorical_variable(
            'city_token',
            'Ciudad del usuario (tokenizada)'
        )

        results['r_segment'] = self.analyze_categorical_variable(
            'r_segment',
            'Segmento R del usuario (de otra l√≠nea de negocio)'
        )

        # An√°lisis de variables temporales
        print("\n" + "="*80)
        print("AN√ÅLISIS DE VARIABLES TEMPORALES")
        print("="*80)

        results['first_order_date'] = self.analyze_temporal_variable(
            'first_order_date',
            'Fecha de la primera orden del usuario'
        )

        results['fourth_order_date'] = self.analyze_temporal_variable(
            'fourth_order_date',
            'Fecha de la cuarta orden del usuario'
        )

        # Resumen ejecutivo
        self.generate_executive_summary(results)

        print("\n" + "="*80)
        print("‚úÖ An√°lisis univariado completado")
        print("="*80)

        return results

    def generate_executive_summary(self, results):
        """Genera un resumen ejecutivo del an√°lisis univariado"""
        print("\n" + "="*80)
        print("RESUMEN EJECUTIVO - AN√ÅLISIS UNIVARIADO")
        print("="*80)

        print(f"""
üìä Variables Num√©ricas - Hallazgos Principales:

   1. Total de √ìrdenes:
      - Media: {results['total_orders']['mean']:.1f} √≥rdenes
      - Mediana: {results['total_orders']['median']:.0f} √≥rdenes
      - CV: {results['total_orders']['cv']:.1f}% ({'alta' if results['total_orders']['cv'] > 30 else 'moderada' if results['total_orders']['cv'] > 15 else 'baja'} variabilidad)
      - Asimetr√≠a: {'Positiva (cola derecha)' if results['total_orders']['skewness'] > 0 else 'Negativa (cola izquierda)'}

   2. Delta de √ìrdenes:
      - Media: {results['delta_orders']['mean']:.1f} √≥rdenes
      - Mediana: {results['delta_orders']['median']:.0f} √≥rdenes
      - CV: {results['delta_orders']['cv']:.1f}% ({'alta' if results['delta_orders']['cv'] > 30 else 'moderada' if results['delta_orders']['cv'] > 15 else 'baja'} variabilidad)

   3. EFO-to-Four (Velocidad de Adopci√≥n):
      - Media: {results['efo_to_four']['mean']:.1f} d√≠as
      - Mediana: {results['efo_to_four']['median']:.0f} d√≠as
      - CV: {results['efo_to_four']['cv']:.1f}% ({'alta' if results['efo_to_four']['cv'] > 30 else 'moderada' if results['efo_to_four']['cv'] > 15 else 'baja'} variabilidad)

üìä Variables Categ√≥ricas - Hallazgos Principales:

   1. Categor√≠a de Recencia:
      - Valores √∫nicos: {results['categoria_recencia']['unique_values']}
      - Categor√≠a m√°s com√∫n: {results['categoria_recencia']['mode']}
      - Diversidad Shannon: {results['categoria_recencia']['shannon_evenness']:.2f} ({'alta' if results['categoria_recencia']['shannon_evenness'] > 0.8 else 'moderada' if results['categoria_recencia']['shannon_evenness'] > 0.5 else 'baja'})

   2. Ciudad:
      - Valores √∫nicos: {results['city_token']['unique_values']}
      - Ciudad m√°s com√∫n: {results['city_token']['mode']}
      - Diversidad Shannon: {results['city_token']['shannon_evenness']:.2f}

   3. Segmento R:
      - Valores √∫nicos: {results['r_segment']['unique_values']}
      - Segmento m√°s com√∫n: {results['r_segment']['mode']}
      - Diversidad Shannon: {results['r_segment']['shannon_evenness']:.2f}

üìÖ Variables Temporales - Hallazgos Principales:

   1. Rango de fechas de primera orden: {results['first_order_date']['range_days']} d√≠as
   2. Rango de fechas de cuarta orden: {results['fourth_order_date']['range_days']} d√≠as
""")


if __name__ == "__main__":
    # Ruta al dataset
    DATASET_PATH = "../dataset_protegido (1).csv"

    # Crear instancia del analizador
    analyzer = UnivariateAnalyzer(DATASET_PATH)

    # Ejecutar an√°lisis completo
    results = analyzer.run_full_analysis()

    print("\nüí° Pr√≥ximos pasos recomendados:")
    print("   1. Revisar distribuciones no normales para transformaciones")
    print("   2. Investigar variables con alta variabilidad")
    print("   3. Proceder con an√°lisis multivariado y correlaciones")
